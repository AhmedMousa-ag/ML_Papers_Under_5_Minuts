# TimeVAE: A Variational Auto-Encoder For Multivariate Time Series Generation

Authors: Abhyuday Desai, Cynthia Freeman & Ian Beaver, Zuhui Wang.

Authors propose Variational Auto-Encoders (VAEs) for time series, with distinct properties of: * interpretability, * ability to encode domain knowledge and * reduced training times.

## Introduction

Generative models fall under two categories: 1- trained generators that learn from real-world data, 2- generators that use Monte Carlo sampling methods with use-defined distributions.

The major benifit of trained generators is that they can faithfully represent the patterns observed in real-world data without requiring manual analysis. but thei primary drawback is they can require large training data sets and long training times. 

Monte Carlo methods on the other hand, while being manual to set up, are simpler and more convenient to use. the generated data from them is interpretable and these methods also enable users to control the generated data, inject subject matter expertise, and somulate specific situations. The limitation is that generated data can be substantially different from real-world data.

Therefore a hybrid method that combines both approaches would be valuable for many downstream tasks.

In order for authors to leverage the strengths of both general data generation approaches, they propose a new model based on Variational Auto-Encoders (VAEs) with a decoder design that enables user-defined distributions, which refer to TimeVAE.

## Methods

### Variational Auro-Encoder as Generative Model

The encoder in a VAE outputs the distribution of the embedding instead of a point estimate as in a traditional Auto-Encoder. given an input dataset $X$ which consists of $N$ i.i.d. sample of some continuous or discrete variable $x$. The main goal is to generate samples that accurately represent distributions in original data. specifically modeling the unknown probability function $p(x)$.

The VAE described by Kingma & Welling assumes that the observed data re generated by two step-process. First, a value $z$ is generated from some prior distribution $pθ(z)$, and second, a value $x$ is generated from some conditional distribution $pθ(x|z)$. While true values of the prior $pθ(z)$ and the likelihood $pθ(x|z)$ are unknown to us, we assume they are differentiable WRT both $θ$ and $z$. Now we can define the relationships between the input data and the latent representation as follows: the prior is $pθ(z)$, the likelihood is $pθ(x|z)$, and the posterior is $pθ(z|x)$.

The computation of $pθ(z)$ is very expensive and quite commonly intractable. To overcome this the VAE introduces an approzimation of the posterior distribution as follows: $q Φ (z|x) ≈ p θ (z|x)$, with this framework, the encoder serves to model the probabilistic posterior distrbution $q Φ (z|x)$, while the decoder serves to model the conditional likelihood $p θ (z|x)$.

Typically, this prior distribution of $z$ is chosen to be the Gaussian distribution, more specifically, the standard normal. Then the posterior is regularized during training to ensure that the latent space is wrapped around this prior. that can be done by adding the KL divergence between the variational approximation of the posterior and the chosen prior to the loss function.

### VAE Loss Function

The loss function for the VAE, also named Evidence Lower Bound loss function (ELBO), can be written as follows: $L θ,φ = −E q φ (z|x) [logp θ (x|z)] + D KL (q φ (z|x)||p θ (z))$
The first term on the RHS is the negative log-likelihood of our data given z sampled from $q φ (z|x)$. The second term on the RHS is the KL-Divergence loss between the encoded latent space distribution and the prior. Note that the process involves sampling z from q φ (z|x) which would normally make the operation non-differentiable. However, VAEs use the so-called reparameterization trick which makes the VAE end-to-end trainable (Kingma & Welling, 2013).

### Base TimeVAE Architecture

$ Figure 1

Note that we have not yet described the encoding and decoding functions. One may choose any models for these as long as the loss function described above is differentiable. authors method uses a combination of traditional deep learning layers.Figure 1 provides a block-diagram of the base version of TimeVAE. The base version
excludes the custom temporal structures. It does not require any time-series specific knowledge. 

The input signal X into the encoder is a 3-dimensional array of size $N × T × D$, where $N$ is the batch size, $T$ is the number of time steps, and $D$ is number of feature dimensions. If given data have variable length sequences, then the sequences are padded with zeroes at the beginning to ensure all sequences have same length $T$. The encoder passes the inputs through a series of convolutional layers with ReLU activation. Next, the data are flattened before passing through a fully-connected (dense) linear layer. If $m$ is the number of chosen latent dimensions, representing dimensions of the multi-variate Gaussian, then this last layer has $2m$ number of neurons. they use the output to parameterize the multivariate Gaussian. The size of latent space $m$ is a key model hyper-parameter. Next, we sample the vector $z$ from the multivariate Gaussian using the reparameterization trick. 

The decoder takes the sampled latent vector $z$ which is of length $m$. It is passed through a fully-connected linear layer. Then the data are reshaped into 3-dimensional array before passing through a series of transposed convolutional layers with ReLU activation. Finally, the data passes through a time-distributed fully connected layer with dimensions such that the final output shape is the same as the original signal $X$.

### Interpretable TimeVAE

$Figure 2

$Figure 3

Interpretability of the modeled data generation process can be acheived by injecting temporal structures to the data generation process in the decoder. The Interpretable TimeVAE uses the same encoder structure as that of the Base TimeVAE.

The decoder uses parallel blocks representing different temporal structure that are added together to produce the final output.

**Trend Block:** Authors reuse the trend architecture developed for the N-Beats forecasting model.

Let $P$ be the number of degrees of polynomial specified by the user. They model the trend polynomials $p = 0, 1, 2, .., P$ as a two step process. First, they use the latent space vector $z$ to estimate the basis expansion coefficients $θ_{tr}$ for trend. $θ_{tr}$ is of dimensionality $N × D × P$ . Next, we use $θ_{tr}$ to reconstruct the trend $V_{tr}$ in the original signal. The trend reconstruction in the signal in matrix form is as follows:

$V_{tr} = θ_{tr} R$

$R = [1, r, ..., r^p ]$ is the matrix of powers of $r$ where $r = [0, 1, 2, ..., T − 1]/T$ is a time vector. $R$ is of dimensionality $P × T$ . We perform a matrix multiplication of $θ_{tr}$ and $R$ and then transpose axes 1 and 2, resulting in final trend matrix $V_{tr}$ which has the dimensionality $N × T × D$. The matrix $θ_{tr}$ renders interpretability to the Trend block. Values from $θ_{tr}$ specifically define the $0^{th} , 1^{st} , 2^{nd} , ...P^{th}$ order trend for each sample $N$ and feature dimension $D$.
Note that at $p = 0$, we get a flat trend (i.e. no upward/downward trend), which is equivalent to the level component in traditional time-series modeling nomenclature. Level refers to the average value of the series.

**Seasonality Block:** Let $S$ be the number of different seasonality patterns to be modeled. Each seasonality pattern, indexed by $j$, is defined by two parameters: $m$ as the number of seasons, and $d$ as the duration of each season. On the other hand, for hourly level data, day-of-the-week seasonality is modeled with $m$ equal to 7 and $d$ equal to 24.
For each seasonality pattern $j$, TimeVAE performs two steps. First, the latent space vector $z$ is used to estimate the matrix of basis expansion coefficients $θ^j_{sn}$ which has the dimensionality  $N × D × m$. Next we index the elements in $θ^j_{sn}$ corresponding to specific season for each time step of $X$ to retrieve the seasonal pattern values $V^j_{sn}$ which is of shape $N × T × D$. The final seasonality estimates $V^j_{sn}$ are the element-wise summation of all $V^j_{sn}$ over $j = 1, 2, ..., S$. 

In the seasonal block $j$, the matrix $θ^j_{sn}$ provides interpretability of the seasonal pattern for each sample $N$ and feature dimension $D$. One can index elements in $θ^j_{sn}$ the $m$ seasons within the seasonal cycle.

**Base Decoder as a Residual Block:** The Interpretable TimeVAE architecture also allows use of the original Base Decoder (Figure 1) as a residual branch in the decoder. In fact, one may choose to enable or disable any of the Trend, Seasonality, or Base Decoder branches shown in Figure 2.

**Interpretable Decoder Output:** The final output from the Interpretable Decoder is the element wise summation of the Trend block output $V_{tr}$ , seasonality block outputs $V^j_{sn}$ for $j = 1, 2, ..., S,$ and the output from the residual Base Decoder (if used).

#### TimeVAE Objective Function

Authors train TimeVAE using the ELBO loss function defined earlier with one modification. They use a weight on the reconstruction error which works to increase or decrease the emphasis placed on the reconstruction loss compared to the KL-Divergence loss between approximated posterior $q φ (z|x)$ and the prior $p θ (z)$. Weight factor on reconstruction error ranged between 0.5 and 3.5 during our following experiments and can be chosen by visually inspecting quality of generated samples, or through hyper-parameter tuning if the generated samples are used in a supervised learning task in a downstream application.

## Comparison

$Table 1
